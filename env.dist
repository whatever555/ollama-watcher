# Ollama Configuration
# Copy this file to .env and update with your settings

# Ollama host (default: http://localhost)
OLLAMA_HOST=http://localhost

# Ollama port (default: 11434)
OLLAMA_PORT=11434

# Ollama model to use for code review (default: llama2)
# You can check available models with: ollama list
OLLAMA_MODEL=llama2

# Alternative: DEFAULT_MODEL (fallback if OLLAMA_MODEL is not set)
DEFAULT_MODEL=llama2
